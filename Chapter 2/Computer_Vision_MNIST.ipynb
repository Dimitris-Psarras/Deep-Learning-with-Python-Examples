{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ccabeb9a",
   "metadata": {},
   "source": [
    "# Deep Learning for computer vision\n",
    "\n",
    "The problem we’re trying to solve here is to classify grayscale images of handwritten digits (28 × 28 pixels) into their 10 categories (0 through 9). We’ll use the MNIST dataset, it’s a set of 60,000 training images, plus 10,000 test images, assembled by the National Institute of Standards and Technology (the NIST in MNIST) in the 1980s. \n",
    "\n",
    "First we load the MINST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e4e5216b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import mnist\n",
    "\n",
    "(train_images_orig, train_labels_orig), (test_images_orig, test_labels_orig) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b422a214",
   "metadata": {},
   "source": [
    "Let's look at the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dd4b9e00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_images_orig.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8a739cc5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60000"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_labels_orig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0429b6e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5, 0, 4, ..., 5, 6, 8], dtype=uint8)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels_orig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bba72cab",
   "metadata": {},
   "source": [
    "Here is the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "00fab4fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 28, 28)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_images_orig.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1a472585",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_labels_orig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "43716335",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([7, 2, 1, ..., 4, 5, 6], dtype=uint8)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_labels_orig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09344f2b",
   "metadata": {},
   "source": [
    "## Dense network\n",
    "\n",
    "First we will build a densly connected network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1e45e177",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import models\n",
    "from keras import layers\n",
    "\n",
    "network = models.Sequential()\n",
    "network.add(layers.Dense(512, activation='relu', input_shape=(28*28,)))\n",
    "network.add(layers.Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d6a0347c",
   "metadata": {},
   "outputs": [],
   "source": [
    "network.compile(optimizer='adam',\n",
    "             loss='categorical_crossentropy',\n",
    "             metrics=['acc'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ca36d5d",
   "metadata": {},
   "source": [
    "Before training, we’ll preprocess the data by reshaping it into the shape the network expects and scaling it so that all values are in the [0, 1] interval. Previously, our training images, for instance, were stored in an array of shape (60000, 28, 28) of type uint8 with values in the [0, 255] interval. We transform it into a float32 array of shape (60000, 28 * 28) with values between 0 and 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3c5b2169",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images = train_images_orig.reshape((60000,28*28))\n",
    "train_images = train_images.astype('float32') / 255\n",
    "\n",
    "test_images = test_images_orig.reshape((10000,28*28))\n",
    "test_images = test_images.astype('float32') / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a1fe319b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 784)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_images.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8ddfe8b",
   "metadata": {},
   "source": [
    "Next we categorical encode the labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1e3eecc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical\n",
    "\n",
    "train_labels = to_categorical(train_labels_orig)\n",
    "test_labels = to_categorical(test_labels_orig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a471df6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [1., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 1., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0f5b0738",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 10)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aa6d201",
   "metadata": {},
   "source": [
    "Next we train the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e33641a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.2688 - acc: 0.9244\n",
      "Epoch 2/5\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.1057 - acc: 0.9688\n",
      "Epoch 3/5\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.0697 - acc: 0.9793\n",
      "Epoch 4/5\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.0495 - acc: 0.9859\n",
      "Epoch 5/5\n",
      "469/469 [==============================] - 2s 4ms/step - loss: 0.0353 - acc: 0.9897\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1e2d131d580>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "network.fit(train_images,train_labels, epochs=5, batch_size=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ad2618f",
   "metadata": {},
   "source": [
    "Next we check how the network performs on the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2d73e8dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 1s 1ms/step - loss: 0.0682 - acc: 0.9787\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = network.evaluate(test_images,test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dcee428f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test accuracy:0.9786999821662903 and test loss:0.06821893900632858\n"
     ]
    }
   ],
   "source": [
    "print(f'test accuracy:{test_acc} and test loss:{test_loss}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c272759d",
   "metadata": {},
   "source": [
    "The test-set accuracy turns out to be 97.8%—that’s quite a bit lower than the training set accuracy. This gap between training accuracy and test accuracy is an example of overfitting: the fact that machine-learning models tend to perform worse on new data than on their training data\n",
    "\n",
    "## Convolutional Network (Convnets)\n",
    "\n",
    "Now let's use convnet to classify the MNIST digits. The following lines of code show you what a basic convnet looks like. It’s a stack of Conv2D and MaxPooling2D layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0eeeae04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 26, 26, 32)        320       \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 13, 13, 32)       0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 11, 11, 64)        18496     \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 5, 5, 64)         0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 3, 3, 64)          36928     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 55,744\n",
      "Trainable params: 55,744\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = models.Sequential()\n",
    "model.add(layers.Conv2D(32,(3,3),activation='relu', input_shape=(28,28,1)))\n",
    "model.add(layers.MaxPooling2D((2,2)))\n",
    "model.add(layers.Conv2D(64,(3,3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2,2)))\n",
    "model.add(layers.Conv2D(64,(3,3),activation='relu'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e1c2c68",
   "metadata": {},
   "source": [
    "The next step is to feed the last output tensor (of shape (3, 3, 64)) into a densely connected classifier network like those you’re already familiar with: a stack of Dense layers. These classifiers process vectors, which are 1D, whereas the current output is a 3D tensor. First we have to flatten the 3D outputs to 1D, and then add a few Dense layers on top."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2fa0b33b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(64, activation='relu'))\n",
    "model.add(layers.Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e556d78a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 26, 26, 32)        320       \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 13, 13, 32)       0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 11, 11, 64)        18496     \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 5, 5, 64)         0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 3, 3, 64)          36928     \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 576)               0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 64)                36928     \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 10)                650       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 93,322\n",
      "Trainable params: 93,322\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79a37ee1",
   "metadata": {},
   "source": [
    "Now, let’s train the convnet on the MNIST digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c8c9b50d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images_conv = train_images_orig.reshape((60000,28,28,1))\n",
    "train_images_conv = train_images_conv.astype('float32') / 255\n",
    "\n",
    "test_images_conv = test_images_orig.reshape((10000,28,28,1))\n",
    "test_images_conv = test_images_conv.astype('float32') / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d88822d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "938/938 [==============================] - 18s 19ms/step - loss: 0.1749 - acc: 0.9466\n",
      "Epoch 2/5\n",
      "938/938 [==============================] - 18s 19ms/step - loss: 0.0521 - acc: 0.9841\n",
      "Epoch 3/5\n",
      "938/938 [==============================] - 18s 19ms/step - loss: 0.0365 - acc: 0.9888\n",
      "Epoch 4/5\n",
      "938/938 [==============================] - 18s 19ms/step - loss: 0.0274 - acc: 0.9912\n",
      "Epoch 5/5\n",
      "938/938 [==============================] - 18s 19ms/step - loss: 0.0234 - acc: 0.9924\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1e2d2a94580>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(optimizer='adam',\n",
    "             loss='categorical_crossentropy',\n",
    "             metrics=['acc'])\n",
    "model.fit(train_images_conv,train_labels,epochs=5,batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "905c4c29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 1s 4ms/step - loss: 0.0320 - acc: 0.9905\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9904999732971191"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_loss_conv, test_acc_conv = model.evaluate(test_images_conv,test_labels)\n",
    "test_acc_conv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edf9ce83",
   "metadata": {},
   "source": [
    "Whereas the densely connected network had a test accuracy of 97.8%, the basic convnet has a test accuracy of 99.3%: we decreased the error rate by 68% (relative). Not bad!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
